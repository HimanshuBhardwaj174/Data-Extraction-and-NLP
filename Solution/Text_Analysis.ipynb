{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32dcf8b2-5c9e-46b5-b1a6-9ae2cb5556fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import syllapy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938a7ac0-9527-49bf-9e57-2a815a7b6b44",
   "metadata": {},
   "source": [
    "## Positive and Negative Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "098e8afb-249b-4784-bee4-b7b6eddd976f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../MasterDictionary/positive-words.txt', 'r') as file:\n",
    "            # Read the contents of the file\n",
    "            text = file.read()\n",
    "            # Split the text into words\n",
    "            p_words = text.split()\n",
    "    \n",
    "with open('../MasterDictionary/negative-words.txt', 'r') as file:\n",
    "            # Read the contents of the file\n",
    "            text = file.read()\n",
    "            # Split the text into words\n",
    "            n_words = text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a809a63d-8cdb-48ae-9401-3ac8f970eaaa",
   "metadata": {},
   "source": [
    "## Stopwords Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c9efd3b-dbc9-4305-8c4d-12765be9732c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stopword_filepath = os.listdir('../StopWords/')\n",
    "# Create an empty list to store the words\n",
    "words = []\n",
    "for file in all_stopword_filepath:\n",
    "    filepath = os.path.join('../StopWords/',file)\n",
    "    with open(filepath, 'r') as file:\n",
    "        text = file.read()\n",
    "        word= text.split()\n",
    "        # Loop through each line in the file\n",
    "        for line in word:\n",
    "            if line=='|':continue\n",
    "            words.append(line.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "016331e4-d8a8-40b2-b62b-3f16bcad99e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_from_drive=[]\n",
    "for i in words:\n",
    "    stopword_from_drive.append(i.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e052bb-227b-4965-8422-850807606939",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf8df5b-aa5b-44ca-9436-e89214827b61",
   "metadata": {},
   "source": [
    "## Stopword Removal Function and Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2aa1e03-5c9f-4659-beb8-e26af7803e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Remove Stopword from the text ,with reference to stopwords given in drive\n",
    "def remove_stopword_drive(text):\n",
    "    new_text=[]\n",
    "    token_word = word_tokenize(text)\n",
    "    for i in token_word:\n",
    "        if i in stopword_from_drive:\n",
    "            pass\n",
    "        else:\n",
    "            new_text.append(i)\n",
    "    return new_text\n",
    "\n",
    "##Remove Stopword with reference to nltk datasets\n",
    "def remove_stopword_nltk(text):\n",
    "    new_text=[]\n",
    "    for i in text:\n",
    "        if i in stopwords.words('english'):\n",
    "            pass\n",
    "        else:\n",
    "            new_text.append(i)\n",
    "    return new_text\n",
    "\n",
    "def Remove_punctuation(text):\n",
    "    new_text=[]\n",
    "    exclude = string.punctuation\n",
    "    for i in text:\n",
    "        if i in exclude:\n",
    "            pass\n",
    "        else:\n",
    "            new_text.append(i)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9701de60-f54f-421d-b071-426a543eaf6a",
   "metadata": {},
   "source": [
    "## Score- Positive,Negative,Polarity,Subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "850ac4c0-f285-4b29-b820-9f4b7d4a88fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positive_score(tokenize_text):\n",
    "    positive_val = 0  # initiate with 0\n",
    "    for word in tokenize_text:\n",
    "        if word in p_words:    \n",
    "            positive_val += 1\n",
    "    return positive_val\n",
    "\n",
    "def negative_score(tokenize_text):\n",
    "    negative_val = 0  # initiate with 0\n",
    "    for word in tokenize_text:\n",
    "        if word in n_words:     \n",
    "            negative_val -= 1\n",
    "    return negative_val\n",
    "\n",
    "def polarity_score(tokenize_text):\n",
    "    positive_value = positive_score(tokenize_text)\n",
    "    negative_value = negative_score(tokenize_text)\n",
    "    polarity_score = (positive_value - negative_value) / ((positive_value + negative_value) + 0.000001)\n",
    "    return polarity_score\n",
    "\n",
    "def subjectivity_score(tokenize_text):\n",
    "    positive_value = positive_score(tokenize_text)\n",
    "    negative_value = negative_score(tokenize_text)\n",
    "    subjectivity_score = (positive_value + negative_value) / (len(tokenize_text) + 0.000001)\n",
    "    return subjectivity_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca822c2-e12a-444f-aa30-a49c1e42d62c",
   "metadata": {},
   "source": [
    "## Average Sentence Length and Word length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dd6492c-60fd-4596-b463-cfc287fc6dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Avg_Sentence_length(text,no_words):\n",
    "    sentences = re.split(r'[.?\\n]', text)\n",
    "    ##For removing ' '\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    Avg_sen_length = len(no_words)/(len(sentences))\n",
    "    return Avg_sen_length\n",
    "\n",
    "def Avg_Word_Sentence(text,no_words):\n",
    "    sentences = re.split(r'[.?\\n]', text)\n",
    "    ##For removing ' '\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    Avg_sen_length = len(no_words)/(len(sentences))\n",
    "    return Avg_sen_length\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94680c2f-ec99-4fdd-8791-466c9b014af0",
   "metadata": {},
   "source": [
    "## Complex words and Fog Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1fb30d6-5944-4065-ba54-362de8b2a4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_complex_words(text):\n",
    "    complex_word_count = 0\n",
    "    for word in text:\n",
    "        # Calculate the number of syllables in the word\n",
    "        syllable_count = syllapy.count(word)\n",
    "        # Define a threshold for what constitutes a complex word\n",
    "        # For example, words with more then 2 syllables may be considered complex\n",
    "        if syllable_count > 2:\n",
    "            complex_word_count += 1\n",
    "\n",
    "    return complex_word_count\n",
    "\n",
    "def percentage_complex_words(text):\n",
    "    return (count_complex_words(text)/len(text))\n",
    "\n",
    "def fog_index(avg_sentence_length,percentage_complex_no):\n",
    "    return 0.4*(avg_sentence_length+percentage_complex_no)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98164b91-6ca1-4139-966a-60aedb85c412",
   "metadata": {},
   "source": [
    "## Syllables per word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c72cad3-519a-425c-a925-bf6ccc99a340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_syllables(word):\n",
    "    vowel='aeiou'\n",
    "    num_vowel = 0\n",
    "    for i in word:\n",
    "        if i in vowel:\n",
    "            num_vowel+=1\n",
    "    # Substracting the Addition of vowel due to ed and es\n",
    "    if word.endswith('ed') or word.endswith('es'):\n",
    "        num_vowel -=1 \n",
    "    return num_vowel\n",
    "\n",
    "def count_syllables_per_word(text):\n",
    "    ##Creating list of vowel per word\n",
    "    counts = [num_syllables(word) for word in text]\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c963aca-a1c1-4ed5-91a0-d2d65f0937d2",
   "metadata": {},
   "source": [
    "## Personal Pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37556e60-5fc2-410a-8370-48de8ff1b1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_personal_pronous(whole_text):\n",
    "    pattern = re.compile(r'(?<!\\bUS\\b)\\b(I|we|my|ours|us)\\b', re.IGNORECASE)\n",
    "    matches = pattern.findall(whole_text)\n",
    "    return len(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d749d71-fc07-4ec6-a6a5-85dc3cbe4322",
   "metadata": {},
   "source": [
    "## Average Word Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a9d0acb-0bc6-4ef4-b8ee-f324b4d8dcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word_length(text):\n",
    "    total_char = 0\n",
    "    total_word = len(text)\n",
    "\n",
    "    for word in text:\n",
    "        total_char+=len(word)\n",
    "    return total_char/total_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbad6de-db4b-4474-8b87-696d6e0c5e96",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4aff50f5-ce64-4903-b553-82cd2027ca75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\himan\\AppData\\Local\\Temp\\ipykernel_26232\\2823246485.py:46: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df,df2])\n"
     ]
    }
   ],
   "source": [
    "input = pd.read_excel('../Input.xlsx')\n",
    "URLS= input['URL']\n",
    "URLS_ID = input['URL_ID']\n",
    "\n",
    "Data = [\n",
    "    'URL_ID','URL','POSITIVE SCORE','NEGATIVE SCORE','POLARITY SCORE','SUBJECTIVITY SCORE','AVG SENTENCE LENGTH',\n",
    "    'PERCENTAGE OF COMPLEX WORDS','FOG INDEX','AVG NUMBER OF WORDS PER SENTENCE','COMPLEX WORD COUNT',\n",
    "    'WORD COUNT','SYLLABLE PER WORD','PERSONAL PRONOUNS','AVG WORD LENGTH'\n",
    "]\n",
    "df=pd.DataFrame(columns=Data)\n",
    "\n",
    "file = os.listdir('./Text_File/')\n",
    "mark = -1\n",
    "for i in file:\n",
    "    mark+=1\n",
    "    ## To fix the numbering due to page not found during web scraping\n",
    "    if URLS_ID[mark] == 'blackassign0036' or URLS_ID[mark] == 'blackassign0049':\n",
    "        mark+=1\n",
    "    file_path = os.path.join('./Text_File/',i)\n",
    "    with open(file_path,encoding='utf-8') as f:\n",
    "        text_for_pronouns= f.read().strip() # Creating new text for pronous count (with stopwords)\n",
    "        text = text_for_pronouns.lower() # Implementing lower beacause to remove stopwords and punctuation\n",
    "        tokenize_1 = remove_stopword_drive(text) #tokenize word after removing stopword (comparing with drive)\n",
    "        tokenize_2 = remove_stopword_nltk(tokenize_1) #tokenize word after removing stopword(comparing with nltk data)\n",
    "        tokenize_3 = Remove_punctuation(tokenize_2) #tokenize word after removing punctuation\n",
    "        \n",
    "        dict ={\n",
    "            'URL_ID': [URLS_ID[mark]],\n",
    "            'URL': [URLS[mark]],\n",
    "            'POSITIVE SCORE' : [positive_score(tokenize_3)] ,\n",
    "            'NEGATIVE SCORE' : [negative_score(tokenize_3)],\n",
    "            'POLARITY SCORE' : [polarity_score(tokenize_3)],\n",
    "            'SUBJECTIVITY SCORE' : [subjectivity_score(tokenize_3)],\n",
    "            'AVG SENTENCE LENGTH' : [Avg_Sentence_length(text,tokenize_3)],\n",
    "            'PERCENTAGE OF COMPLEX WORDS' : [percentage_complex_words(tokenize_3)],\n",
    "            'FOG INDEX' : [fog_index(Avg_Sentence_length(text,tokenize_3),percentage_complex_words(tokenize_3))],\n",
    "            'AVG NUMBER OF WORDS PER SENTENCE' : [Avg_Word_Sentence(text,tokenize_3)],\n",
    "            'COMPLEX WORD COUNT' : [count_complex_words(tokenize_3)],\n",
    "            'WORD COUNT' : [len(tokenize_3)],\n",
    "            'SYLLABLE PER WORD' : [count_syllables_per_word(tokenize_3)],\n",
    "            'PERSONAL PRONOUNS' : [count_personal_pronous(text)],\n",
    "            'AVG WORD LENGTH' : [avg_word_length(tokenize_3)]\n",
    "        }\n",
    "\n",
    "        df2 = pd.DataFrame(dict)\n",
    "        df = pd.concat([df,df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee18ee2f-6845-4c55-b5d2-c9d3fec4778d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>26</td>\n",
       "      <td>-6</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.041237</td>\n",
       "      <td>5.511364</td>\n",
       "      <td>0.323711</td>\n",
       "      <td>2.334030</td>\n",
       "      <td>5.511364</td>\n",
       "      <td>157</td>\n",
       "      <td>485</td>\n",
       "      <td>[2, 2, 3, 4, 5, 2, 0, 2, 4, 4, 3, 3, 2, 2, 4, ...</td>\n",
       "      <td>12</td>\n",
       "      <td>6.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>53</td>\n",
       "      <td>-31</td>\n",
       "      <td>3.818182</td>\n",
       "      <td>0.032211</td>\n",
       "      <td>8.130952</td>\n",
       "      <td>0.478770</td>\n",
       "      <td>3.443889</td>\n",
       "      <td>8.130952</td>\n",
       "      <td>327</td>\n",
       "      <td>683</td>\n",
       "      <td>[2, 2, 3, 4, 5, 2, 3, 2, 4, 5, 0, 2, 4, 3, 3, ...</td>\n",
       "      <td>6</td>\n",
       "      <td>7.682284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>https://insights.blackcoffer.com/internet-dema...</td>\n",
       "      <td>36</td>\n",
       "      <td>-23</td>\n",
       "      <td>4.538461</td>\n",
       "      <td>0.020934</td>\n",
       "      <td>9.553846</td>\n",
       "      <td>0.552335</td>\n",
       "      <td>4.042472</td>\n",
       "      <td>9.553846</td>\n",
       "      <td>343</td>\n",
       "      <td>621</td>\n",
       "      <td>[3, 2, 0, 5, 6, 2, 0, 0, 5, 2, 5, 1, 2, 3, 4, ...</td>\n",
       "      <td>13</td>\n",
       "      <td>8.330113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0004</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "      <td>37</td>\n",
       "      <td>-74</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>-0.061770</td>\n",
       "      <td>9.359375</td>\n",
       "      <td>0.522538</td>\n",
       "      <td>3.952765</td>\n",
       "      <td>9.359375</td>\n",
       "      <td>313</td>\n",
       "      <td>599</td>\n",
       "      <td>[2, 3, 2, 3, 3, 2, 1, 5, 6, 1, 2, 2, 5, 3, 2, ...</td>\n",
       "      <td>5</td>\n",
       "      <td>8.095159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0005</td>\n",
       "      <td>https://insights.blackcoffer.com/ott-platform-...</td>\n",
       "      <td>19</td>\n",
       "      <td>-8</td>\n",
       "      <td>2.454545</td>\n",
       "      <td>0.032070</td>\n",
       "      <td>8.365854</td>\n",
       "      <td>0.457726</td>\n",
       "      <td>3.529432</td>\n",
       "      <td>8.365854</td>\n",
       "      <td>157</td>\n",
       "      <td>343</td>\n",
       "      <td>[2, 2, 5, 2, 3, 0, 2, 2, 3, 5, 1, 5, 2, 4, 2, ...</td>\n",
       "      <td>6</td>\n",
       "      <td>7.930029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0096</td>\n",
       "      <td>https://insights.blackcoffer.com/what-is-the-r...</td>\n",
       "      <td>26</td>\n",
       "      <td>-54</td>\n",
       "      <td>-2.857143</td>\n",
       "      <td>-0.051471</td>\n",
       "      <td>9.890909</td>\n",
       "      <td>0.430147</td>\n",
       "      <td>4.128422</td>\n",
       "      <td>9.890909</td>\n",
       "      <td>234</td>\n",
       "      <td>544</td>\n",
       "      <td>[2, 2, 5, 4, 4, 3, 2, 3, 1, 2, 3, 3, 2, 3, 2, ...</td>\n",
       "      <td>4</td>\n",
       "      <td>7.380515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0097</td>\n",
       "      <td>https://insights.blackcoffer.com/impact-of-cov...</td>\n",
       "      <td>22</td>\n",
       "      <td>-35</td>\n",
       "      <td>-4.384616</td>\n",
       "      <td>-0.031785</td>\n",
       "      <td>10.225000</td>\n",
       "      <td>0.371638</td>\n",
       "      <td>4.238655</td>\n",
       "      <td>10.225000</td>\n",
       "      <td>152</td>\n",
       "      <td>409</td>\n",
       "      <td>[2, 2, 3, 3, 2, 3, 3, 2, 0, 2, 1, 1, 3, 1, 2, ...</td>\n",
       "      <td>7</td>\n",
       "      <td>6.735941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0098</td>\n",
       "      <td>https://insights.blackcoffer.com/contribution-...</td>\n",
       "      <td>5</td>\n",
       "      <td>-3</td>\n",
       "      <td>3.999998</td>\n",
       "      <td>0.009479</td>\n",
       "      <td>6.806452</td>\n",
       "      <td>0.450237</td>\n",
       "      <td>2.902675</td>\n",
       "      <td>6.806452</td>\n",
       "      <td>95</td>\n",
       "      <td>211</td>\n",
       "      <td>[5, 3, 3, 1, 5, 3, 3, 2, 1, 1, 3, 3, 3, 1, 3, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>7.469194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0099</td>\n",
       "      <td>https://insights.blackcoffer.com/how-covid-19-...</td>\n",
       "      <td>11</td>\n",
       "      <td>-3</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>7.733333</td>\n",
       "      <td>0.301724</td>\n",
       "      <td>3.214023</td>\n",
       "      <td>7.733333</td>\n",
       "      <td>70</td>\n",
       "      <td>232</td>\n",
       "      <td>[2, 3, 2, 3, 1, 0, 3, 2, 3, 2, 0, 4, 2, 3, 2, ...</td>\n",
       "      <td>4</td>\n",
       "      <td>6.681034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0100</td>\n",
       "      <td>https://insights.blackcoffer.com/how-will-covi...</td>\n",
       "      <td>28</td>\n",
       "      <td>-52</td>\n",
       "      <td>-3.333333</td>\n",
       "      <td>-0.054670</td>\n",
       "      <td>12.911765</td>\n",
       "      <td>0.387244</td>\n",
       "      <td>5.319603</td>\n",
       "      <td>12.911765</td>\n",
       "      <td>170</td>\n",
       "      <td>439</td>\n",
       "      <td>[2, 2, 1, 1, 3, 2, 4, 2, 4, 2, 1, 1, 2, 2, 2, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>7.230068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             URL_ID                                                URL  \\\n",
       "0   blackassign0001  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "0   blackassign0002  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "0   blackassign0003  https://insights.blackcoffer.com/internet-dema...   \n",
       "0   blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...   \n",
       "0   blackassign0005  https://insights.blackcoffer.com/ott-platform-...   \n",
       "..              ...                                                ...   \n",
       "0   blackassign0096  https://insights.blackcoffer.com/what-is-the-r...   \n",
       "0   blackassign0097  https://insights.blackcoffer.com/impact-of-cov...   \n",
       "0   blackassign0098  https://insights.blackcoffer.com/contribution-...   \n",
       "0   blackassign0099  https://insights.blackcoffer.com/how-covid-19-...   \n",
       "0   blackassign0100  https://insights.blackcoffer.com/how-will-covi...   \n",
       "\n",
       "   POSITIVE SCORE NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \\\n",
       "0              26             -6        1.600000            0.041237   \n",
       "0              53            -31        3.818182            0.032211   \n",
       "0              36            -23        4.538461            0.020934   \n",
       "0              37            -74       -3.000000           -0.061770   \n",
       "0              19             -8        2.454545            0.032070   \n",
       "..            ...            ...             ...                 ...   \n",
       "0              26            -54       -2.857143           -0.051471   \n",
       "0              22            -35       -4.384616           -0.031785   \n",
       "0               5             -3        3.999998            0.009479   \n",
       "0              11             -3        1.750000            0.034483   \n",
       "0              28            -52       -3.333333           -0.054670   \n",
       "\n",
       "    AVG SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
       "0              5.511364                     0.323711   2.334030   \n",
       "0              8.130952                     0.478770   3.443889   \n",
       "0              9.553846                     0.552335   4.042472   \n",
       "0              9.359375                     0.522538   3.952765   \n",
       "0              8.365854                     0.457726   3.529432   \n",
       "..                  ...                          ...        ...   \n",
       "0              9.890909                     0.430147   4.128422   \n",
       "0             10.225000                     0.371638   4.238655   \n",
       "0              6.806452                     0.450237   2.902675   \n",
       "0              7.733333                     0.301724   3.214023   \n",
       "0             12.911765                     0.387244   5.319603   \n",
       "\n",
       "    AVG NUMBER OF WORDS PER SENTENCE COMPLEX WORD COUNT WORD COUNT  \\\n",
       "0                           5.511364                157        485   \n",
       "0                           8.130952                327        683   \n",
       "0                           9.553846                343        621   \n",
       "0                           9.359375                313        599   \n",
       "0                           8.365854                157        343   \n",
       "..                               ...                ...        ...   \n",
       "0                           9.890909                234        544   \n",
       "0                          10.225000                152        409   \n",
       "0                           6.806452                 95        211   \n",
       "0                           7.733333                 70        232   \n",
       "0                          12.911765                170        439   \n",
       "\n",
       "                                    SYLLABLE PER WORD PERSONAL PRONOUNS  \\\n",
       "0   [2, 2, 3, 4, 5, 2, 0, 2, 4, 4, 3, 3, 2, 2, 4, ...                12   \n",
       "0   [2, 2, 3, 4, 5, 2, 3, 2, 4, 5, 0, 2, 4, 3, 3, ...                 6   \n",
       "0   [3, 2, 0, 5, 6, 2, 0, 0, 5, 2, 5, 1, 2, 3, 4, ...                13   \n",
       "0   [2, 3, 2, 3, 3, 2, 1, 5, 6, 1, 2, 2, 5, 3, 2, ...                 5   \n",
       "0   [2, 2, 5, 2, 3, 0, 2, 2, 3, 5, 1, 5, 2, 4, 2, ...                 6   \n",
       "..                                                ...               ...   \n",
       "0   [2, 2, 5, 4, 4, 3, 2, 3, 1, 2, 3, 3, 2, 3, 2, ...                 4   \n",
       "0   [2, 2, 3, 3, 2, 3, 3, 2, 0, 2, 1, 1, 3, 1, 2, ...                 7   \n",
       "0   [5, 3, 3, 1, 5, 3, 3, 2, 1, 1, 3, 3, 3, 1, 3, ...                 0   \n",
       "0   [2, 3, 2, 3, 1, 0, 3, 2, 3, 2, 0, 4, 2, 3, 2, ...                 4   \n",
       "0   [2, 2, 1, 1, 3, 2, 4, 2, 4, 2, 1, 1, 2, 2, 2, ...                 3   \n",
       "\n",
       "    AVG WORD LENGTH  \n",
       "0          6.800000  \n",
       "0          7.682284  \n",
       "0          8.330113  \n",
       "0          8.095159  \n",
       "0          7.930029  \n",
       "..              ...  \n",
       "0          7.380515  \n",
       "0          6.735941  \n",
       "0          7.469194  \n",
       "0          6.681034  \n",
       "0          7.230068  \n",
       "\n",
       "[98 rows x 15 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6296a5be-ff89-468a-98d2-2e058d8fbced",
   "metadata": {},
   "source": [
    "## Saving File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a3a2329c-bd64-4e42-9151-91609be946f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('../Output_File.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ad549d-654b-446e-a346-b7003f9ad6ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5269c8-693a-44e7-9006-f272497e0f37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d78dd4d-fd8e-4928-8935-2402f4bb1e57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65671833-5e6c-415a-8f3b-dcf8edb6f690",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80756c3c-2475-4bd8-b696-8516b533c904",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
